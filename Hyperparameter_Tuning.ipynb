{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4EVjfVjByZMe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nhLB_YwQzGw3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRmrKTyFzKwW"
   },
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    " process of optimizing the hyperparameters of a machine learning model in order to improve its performance\n",
    "\n",
    " **Hyperparameters:**\n",
    " - configuration settings that are not learned from the data but are set prior to the training process\n",
    " - essential aspects of the model architecture and training procedure that influence the learning process\n",
    " - determine key features such as model architecture, learning rate, and model complexity\n",
    "- there are no set rules on which hyperparameters work best nor their optimal or default values\n",
    "-  to find the optimum hyperparameter set. This activity is known as hyperparameter tuning or hyperparameter optimization.\n",
    "\n",
    "\n",
    "### Hyperparameter Tuning Techniques\n",
    "1. Grid Search:\n",
    "Exhaustive search over a predefined hyperparameter grid.\n",
    "Evaluates model performance for all possible combinations.\n",
    "\n",
    "2. Random Search:\n",
    "Randomly samples hyperparameter combinations.\n",
    "More computationally efficient than grid search.\n",
    "\n",
    "3. Bayesian Optimization:\n",
    "Uses probabilistic models to model the objective function.\n",
    "Adapts and focuses the search on promising hyperparameter regions.\n",
    "\n",
    "5. Sequential Model-Based Optimization (SMBO):\n",
    "Combines surrogate model predictions with acquisition functions.\n",
    "Balances exploration-exploitation trade-off efficiently.\n",
    "5. Gradient-Based Optimization:\n",
    "Derivative-based optimization methods.\n",
    "Efficient for continuous hyperparameters but less common in discrete spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\EQ2040AU\\Anaconda\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grid Search:**\n",
    "\n",
    "**Definition:**\n",
    "- Exhaustive search over a predefined hyperparameter grid.\n",
    "- Systematically evaluates all possible combinations of hyperparameter values.\n",
    "\n",
    "**Some Popular Algorithms it is Used With:**\n",
    "- Grid search can be applied to a wide range of machine learning algorithms, including:\n",
    "  1. Support Vector Machines (SVM)\n",
    "  2. Decision Trees\n",
    "  3. Random Forest\n",
    "  4. k-Nearest Neighbors (k-NN)\n",
    "  5. Gradient Boosting algorithms (e.g., XGBoost)\n",
    "\n",
    "**When it is Useful:**\n",
    "- **Simple Hyperparameter Spaces:** Grid search is effective when the hyperparameter space is relatively small and simple.\n",
    "- **Exploring Interactions:** It helps in exploring interactions between hyperparameters.\n",
    "- **Baseline Search:** It provides a good baseline tuning method.\n",
    "\n",
    "**Examples of Usefulness:**\n",
    "- Grid search is useful when tuning hyperparameters like learning rates, regularization strengths, or kernel types.\n",
    "- In a decision tree, it can explore different depths and minimum samples per leaf.\n",
    "\n",
    "**When it is Not Useful:**\n",
    "- **Large Search Spaces:** Grid search becomes impractical when dealing with a large number of hyperparameter combinations.\n",
    "- **Continuous Hyperparameters:** It's less effective when hyperparameters are continuous, as it might miss optimal values.\n",
    "  \n",
    "**Examples of Not Usefulness:**\n",
    "- In deep neural networks with many hyperparameters, exploring all combinations exhaustively can be computationally expensive.\n",
    "- When searching for optimal values of a learning rate in a continuous space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = SVC()\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],          # Regularization parameter\n",
    "    'kernel': ['linear', 'rbf'],    # Kernel type\n",
    "    'gamma': ['scale', 'auto'],     # Kernel coefficient\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 1, 'gamma': 'scale', 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy = grid_search.best_estimator_.score(X_test, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Search:**\n",
    "\n",
    "**Definition:**\n",
    "- Randomly samples hyperparameter combinations from a predefined search space.\n",
    "- Provides a more computationally efficient alternative to exhaustive grid search.\n",
    "\n",
    "**Some Popular Algorithms it is Used With:**\n",
    "- Random search is versatile and can be used with a variety of machine learning algorithms, including:\n",
    "  1. Support Vector Machines (SVM)\n",
    "  2. Decision Trees\n",
    "  3. Random Forest\n",
    "  4. Neural Networks\n",
    "  5. Gradient Boosting algorithms (e.g., XGBoost)\n",
    "\n",
    "**When it is Useful:**\n",
    "- **Large Search Spaces:** Random search is beneficial when dealing with a large number of hyperparameter combinations.\n",
    "- **Efficiency:** It is computationally more efficient than grid search, as it samples a subset of hyperparameter space.\n",
    "- **Exploration:** Useful for exploring diverse regions of the hyperparameter space.\n",
    "\n",
    "**Examples of Usefulness:**\n",
    "- Random search is effective when searching for optimal combinations of hyperparameters like learning rates, regularization strengths, and depths of decision trees.\n",
    "- In neural networks, it can efficiently sample architectures, dropout rates, and batch sizes.\n",
    "\n",
    "**When it is Not Useful:**\n",
    "- **Interactions Between Hyperparameters:** Random search might miss interactions between hyperparameters, as it samples independently.\n",
    "- **Fine-Tuning:** Not suitable for fine-tuning or narrowing down the search space once a general idea is obtained.\n",
    "\n",
    "**Examples of Not Usefulness:**\n",
    "- In scenarios where there are strong interactions between multiple hyperparameters, random search may not explore these relationships thoroughly.\n",
    "- If a more focused search is needed after an initial exploration, random search might not be the best choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=SVC(),\n",
       "                   param_distributions={'C': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x00000258985D73D0>,\n",
       "                                        'gamma': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x00000258F45F73D0>,\n",
       "                                        'kernel': ['linear', 'rbf']})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from scipy.stats import uniform, loguniform\n",
    "\n",
    "svm_model = SVC()\n",
    "\n",
    "param_dist = {\n",
    "    'C': loguniform(1e-3, 1e3),  # Regularization parameter\n",
    "    'kernel': ['linear', 'rbf'],  # Kernel type\n",
    "    'gamma': uniform(0.01, 1.0),  # Kernel coefficient\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=svm_model, param_distributions=param_dist, n_iter=10, cv=5)\n",
    "\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 0.48443840681289646, 'gamma': 0.26461800657107326, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Hyperparameters:\", random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy = random_search.best_estimator_.score(X_test, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayesian Optimization:**\n",
    "\n",
    "**Definition:**\n",
    "- Bayesian optimization is a probabilistic model-based optimization technique.\n",
    "- It models the objective function as a probabilistic surrogate, combining prior knowledge with observed results to make informed decisions about where to sample next in the hyperparameter space.\n",
    "\n",
    "**Algorithms it is Used With:**\n",
    "- Bayesian optimization is versatile and can be used with various machine learning algorithms, including:\n",
    "  1. Support Vector Machines (SVM)\n",
    "  2. Decision Trees\n",
    "  3. Random Forest\n",
    "  4. Gaussian Processes for regression\n",
    "  5. Neural Networks\n",
    "  6. Gradient Boosting algorithms (e.g., XGBoost)\n",
    "\n",
    "**When it is Useful:**\n",
    "- **Expensive Objective Functions:** Useful when evaluating the objective function is computationally expensive.\n",
    "- **Global Optimization:** Efficient for finding global optima in the hyperparameter space.\n",
    "- **Fewer Evaluations:** Requires fewer evaluations compared to exhaustive search methods.\n",
    "\n",
    "**Examples of Usefulness:**\n",
    "- Bayesian optimization is beneficial when optimizing hyperparameters like learning rates, regularization strengths, and kernel parameters.\n",
    "- In scenarios where each evaluation of the objective function (e.g., model training) is time-consuming, such as optimizing hyperparameters of a deep neural network.\n",
    "\n",
    "**When it is Not Useful:**\n",
    "- **Simple Hyperparameter Spaces:** Might be overkill for small and simple hyperparameter spaces.\n",
    "- **Low-Dimensional Spaces:** In low-dimensional spaces, simpler optimization methods like grid search or random search might suffice.\n",
    "\n",
    "**Examples of Not Usefulness:**\n",
    "- When dealing with a very simple model with only a couple of hyperparameters, Bayesian optimization might be too sophisticated.\n",
    "- In scenarios where the objective function is not computationally expensive, simpler optimization methods may provide similar results more efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient-Based Optimization:**\n",
    "\n",
    "**Definition:**\n",
    "- Gradient-based optimization involves using the gradient (partial derivatives) of the objective function with respect to the hyperparameters to guide the search for optimal values.\n",
    "- Iteratively updates hyperparameters in the direction of steepest ascent or descent based on the gradient.\n",
    "\n",
    "**Algorithms it is Used With:**\n",
    "- Gradient-based optimization is commonly used with algorithms that involve differentiable objective functions, such as:\n",
    "  1. Neural Networks (e.g., using stochastic gradient descent)\n",
    "  2. Linear Regression\n",
    "  3. Logistic Regression\n",
    "  4. Support Vector Machines (using techniques like SMO)\n",
    "  5. Linear Discriminant Analysis\n",
    "\n",
    "**When it is Useful:**\n",
    "- **Differentiable Objective Functions:** Effective when the objective function is differentiable, allowing computation of gradients.\n",
    "- **Smooth Surfaces:** Suitable for optimizing smooth, continuous objective functions.\n",
    "- **Local Search:** Efficient for fine-tuning in the vicinity of promising solutions.\n",
    "\n",
    "**Examples of Usefulness:**\n",
    "- Gradient-based optimization is valuable in training deep neural networks by updating weights to minimize the loss function.\n",
    "- In linear regression, it is used to find the coefficients that minimize the sum of squared differences between predicted and actual values.\n",
    "\n",
    "**When it is Not Useful:**\n",
    "- **Discontinuous or Nondifferentiable Functions:** Ineffective when dealing with functions that are not differentiable or have discontinuities.\n",
    "- **Global Optimization:** May get stuck in local minima and struggle to find the global minimum in complex, non-convex spaces.\n",
    "\n",
    "**Examples of Not Usefulness:**\n",
    "- In genetic algorithms or evolutionary strategies, where the objective function might not be differentiable, gradient-based optimization is not suitable.\n",
    "- For hyperparameter tuning in complex models like deep neural networks with non-convex loss surfaces, it might struggle to find the global optimum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='relu', input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = tf.argmax(y_pred, axis=1).numpy()\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sequential Model-Based Optimization (SMBO):**\n",
    "\n",
    "**Definition:**\n",
    "- Sequential Model-Based Optimization (SMBO) is an optimization technique that combines probabilistic surrogate models with acquisition functions to sequentially optimize the objective function.\n",
    "- It iteratively fits surrogate models to the observed data and uses them to propose the next set of hyperparameters to evaluate.\n",
    "\n",
    "**Algorithms it is Used With:**\n",
    "- SMBO can be used with various machine learning algorithms, including:\n",
    "  1. Support Vector Machines (SVM)\n",
    "  2. Decision Trees\n",
    "  3. Random Forest\n",
    "  4. Neural Networks\n",
    "  5. Gradient Boosting algorithms (e.g., XGBoost)\n",
    "\n",
    "**When it is Useful:**\n",
    "- **Expensive Objective Functions:** Effective when evaluating the objective function is computationally expensive.\n",
    "- **Global Optimization:** Efficient for finding global optima in the hyperparameter space.\n",
    "- **Adaptation:** Adapts to the characteristics of the optimization landscape over iterations.\n",
    "\n",
    "**Examples of Usefulness:**\n",
    "- In optimizing hyperparameters like learning rates, regularization strengths, and depths of decision trees.\n",
    "- In scenarios where each evaluation of the objective function, such as training a complex model, is time-consuming.\n",
    "\n",
    "**When it is Not Useful:**\n",
    "- **Simple Hyperparameter Spaces:** Might be overkill for small and simple hyperparameter spaces.\n",
    "- **Low-Dimensional Spaces:** In low-dimensional spaces, simpler optimization methods like grid search or random search might suffice.\n",
    "\n",
    "**Examples of Not Usefulness:**\n",
    "- When dealing with a very simple model with only a couple of hyperparameters, SMBO might be too sophisticated.\n",
    "- In scenarios where the objective function is not computationally expensive, simpler optimization methods may provide similar results more efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "additional info:\n",
    "1. https://aws.amazon.com/what-is/hyperparameter-tuning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
